{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3106d907",
   "metadata": {},
   "source": [
    "1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b060b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed HTML content from https://www.imdb.com/list/ls056092300\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls056092300\"\n",
    "soup = parse_html(url)\n",
    "if soup:\n",
    "    print(f\"Successfully parsed HTML content from {url}\")\n",
    "else:\n",
    "    print(f\"Failed to parse HTML content from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1be347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Ship of Theseus', '2.Iruvar', '3.Kaagaz Ke Phool', '4.Lagaan: Once Upon a Time in India', '5.Pather Panchali', '6.Charulata', '7.Rang De Basanti', '8.Dev.D', '9.3 Idiots', '10.Awaara', '11.Nayakan', '12.Aparajito', '13.Pushpaka Vimana', '14.Pyaasa', '15.Ghatashraddha', '16.Sholay', '17.Aradhana', '18.Do Ankhen Barah Haath', '19.Bombay', '20.Neecha Nagar', '21.Do Bigha Zamin', '22.Garm Hava', '23.Piravi', '24.Mughal-E-Azam', '25.Amma Ariyan', '26.Madhumati', '27.Goopy Gyne Bagha Byne', '28.Gangs of Wasseypur', '29.Guide', '30.Satya', '31.Roja', '32.Mr. India', '33.The Cloud-Capped Star', '34.Harishchandrachi Factory', '35.Masoom', '36.Agneepath', '37.Tabarana Kathe', '38.Zakhm', '39.Dil Chahta Hai', '40.Bhaag Milkha Bhaag', '41.Chupke Chupke', '42.Dilwale Dulhania Le Jayenge', '43.Taare Zameen Par', '44.Ardh Satya', '45.Bhumika', '46.Enthiran', '47.Sadma', '48.Shwaas', '49.Lamhe', '50.Haqeeqat', '51.Shree 420', '52.Kannathil Muthamittal', '53.Hum Aapke Hain Koun..!', '54.Ustad Hotel', '55.Bandit Queen', '56.Lakshya', '57.Black Friday', '58.Manthan', '59.Apoorva Raagangal', '60.English Vinglish', '61.Jewel Thief', '62.Pakeezah', '63.Maqbool', '64.Jis Desh Men Ganga Behti Hai', '65.Sahib Bibi Aur Ghulam', '66.Shatranj Ke Khilari', '67.Narthanasala', '68.Chandni Bar', '69.Vaaranam Aayiram', '70.Mr. and Mrs. Iyer', '71.Chandni', '72.English, August', '73.Celluloid', '74.Sagara Sangamam', '75.Munna Bhai M.B.B.S.', '76.Saaransh', '77.Guddi', '78.Vanaja', '79.Vazhakku Enn 18/9', '80.Gangaajal', '81.Angoor', '82.Guru', '83.Andaz Apna Apna', '84.Sangam ', '85.Oka Oori Katha', '86.Bhuvan Shome', '87.Border ', '88.Parineeta', '89.Devdas', '90.Abohomaan', '91.Kuch Kuch Hota Hai', '92.Pithamagan', '93.Veyyil', '94.Chemmeen', '95.Jaane Bhi Do Yaaro', '96.Apur Sansar', '97.Kanchivaram', '98.Monsoon Wedding', '99.Black', '100.Deewaar']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "MovieName=[]\n",
    "\n",
    "for i in soup.find_all('h3',class_='lister-item-header'):\n",
    "      MovieName.append(i.text.replace('\\n',''))\n",
    "        \n",
    "MovieName\n",
    "MovieNames=[]\n",
    "\n",
    "for m in MovieName:\n",
    "    Movie=re.sub(r\"\\([^()]*\\)\",'',m)\n",
    "    MovieNames.append(Movie)\n",
    "\n",
    "print(MovieNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f0f40",
   "metadata": {},
   "source": [
    "Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a246cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8',\n",
       " '8.4',\n",
       " '7.8',\n",
       " '8.1',\n",
       " '8.2',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '7.9',\n",
       " '8.4',\n",
       " '7.8',\n",
       " '8.6',\n",
       " '8.2',\n",
       " '8.6',\n",
       " '8.3',\n",
       " '7.6',\n",
       " '8.1',\n",
       " '7.6',\n",
       " '8.4',\n",
       " '8.1',\n",
       " '6.6',\n",
       " '8.3',\n",
       " '8',\n",
       " '7.8',\n",
       " '8.1',\n",
       " '7.4',\n",
       " '7.8',\n",
       " '8.7',\n",
       " '8.2',\n",
       " '8.3',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '7.7',\n",
       " '7.8',\n",
       " '8.4',\n",
       " '8.4',\n",
       " '7.6',\n",
       " '8.1',\n",
       " '7.9',\n",
       " '8.1',\n",
       " '8.2',\n",
       " '8.3',\n",
       " '8',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '7.4',\n",
       " '7.1',\n",
       " '8.3',\n",
       " '8.2',\n",
       " '7.2',\n",
       " '7.8',\n",
       " '7.9',\n",
       " '8.3',\n",
       " '7.5',\n",
       " '8.2',\n",
       " '7.5',\n",
       " '7.8',\n",
       " '8.4',\n",
       " '7.6',\n",
       " '7.6',\n",
       " '7.8',\n",
       " '7.9',\n",
       " '7.2',\n",
       " '8',\n",
       " '7.1',\n",
       " '8.1',\n",
       " '7.5',\n",
       " '8.1',\n",
       " '7.6',\n",
       " '8.2',\n",
       " '7.9',\n",
       " '6.7',\n",
       " '7.7',\n",
       " '7.7',\n",
       " '8.8',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '7.2',\n",
       " '7.2',\n",
       " '8.3',\n",
       " '7.8',\n",
       " '8.3',\n",
       " '7.7',\n",
       " '8',\n",
       " '7.3',\n",
       " '7.8',\n",
       " '7.2',\n",
       " '7.9',\n",
       " '7.2',\n",
       " '7.7',\n",
       " '7.5',\n",
       " '7.5',\n",
       " '8.3',\n",
       " '7.9',\n",
       " '7.8',\n",
       " '8.3',\n",
       " '8.5',\n",
       " '8.2',\n",
       " '7.3',\n",
       " '8.1',\n",
       " '8']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rating=[]\n",
    "\n",
    "for i in soup.find_all(\"div\",\"ipl-rating-star small\"):\n",
    "      Rating.append(i.text.replace('\\n',''))\n",
    "        \n",
    "        \n",
    "Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b07cad",
   "metadata": {},
   "source": [
    "Year Of Released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8648bddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2012',\n",
       " '1997',\n",
       " '1959',\n",
       " '2001',\n",
       " '1955',\n",
       " '1964',\n",
       " '2006',\n",
       " '2009',\n",
       " '2009',\n",
       " '1951',\n",
       " '1987',\n",
       " '1956',\n",
       " '1987',\n",
       " '1957',\n",
       " '1977',\n",
       " '1975',\n",
       " '1969',\n",
       " '1957',\n",
       " '1995',\n",
       " '1946',\n",
       " '1953',\n",
       " '1974',\n",
       " '1989',\n",
       " '1960',\n",
       " '1986',\n",
       " '1958',\n",
       " '1969',\n",
       " '2012',\n",
       " '1965',\n",
       " '1998',\n",
       " '1992',\n",
       " '1987',\n",
       " '1960',\n",
       " '2009',\n",
       " '1983',\n",
       " '1990',\n",
       " '1986',\n",
       " '1998',\n",
       " '2001',\n",
       " '2013',\n",
       " '1975',\n",
       " '1995',\n",
       " '2007',\n",
       " '1983',\n",
       " '1977',\n",
       " '2010',\n",
       " '1983',\n",
       " '2004',\n",
       " '1991',\n",
       " '1964',\n",
       " '1955',\n",
       " '2002',\n",
       " '1994',\n",
       " '2012',\n",
       " '1994',\n",
       " '2004',\n",
       " '2004',\n",
       " '1976',\n",
       " '1975',\n",
       " '2012',\n",
       " '1967',\n",
       " '1972',\n",
       " '2003',\n",
       " '1960',\n",
       " '1962',\n",
       " '1977',\n",
       " '1963',\n",
       " '2001',\n",
       " '2008',\n",
       " '2002',\n",
       " '1989',\n",
       " '1994',\n",
       " '2013',\n",
       " '1983',\n",
       " '2003',\n",
       " '1984',\n",
       " '1971',\n",
       " '2006',\n",
       " '2012',\n",
       " '2003',\n",
       " '1982',\n",
       " '2007',\n",
       " '1994',\n",
       " '1964',\n",
       " '1978',\n",
       " '1969',\n",
       " '1997',\n",
       " '2005',\n",
       " '1955',\n",
       " '2009',\n",
       " '1998',\n",
       " '2003',\n",
       " '2006',\n",
       " '1965',\n",
       " '1983',\n",
       " '1959',\n",
       " '2008',\n",
       " '2001',\n",
       " '2005',\n",
       " '1975']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Year=[]\n",
    "for i in soup.find_all(\"span\",class_=\"lister-item-year text-muted unbold\"):\n",
    "    Year.append(i.text.replace('(','').replace(')','').replace('I ',''))\n",
    "\n",
    "Year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb922310",
   "metadata": {},
   "source": [
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ceba471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieNames</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.Ship of Theseus</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.Iruvar</td>\n",
       "      <td>1997</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.Kaagaz Ke Phool</td>\n",
       "      <td>1959</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.Lagaan: Once Upon a Time in India</td>\n",
       "      <td>2001</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.Pather Panchali</td>\n",
       "      <td>1955</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.Apur Sansar</td>\n",
       "      <td>1959</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.Kanchivaram</td>\n",
       "      <td>2008</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.Monsoon Wedding</td>\n",
       "      <td>2001</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.Black</td>\n",
       "      <td>2005</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.Deewaar</td>\n",
       "      <td>1975</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             MovieNames YearOfRelease Rating\n",
       "0                     1.Ship of Theseus          2012      8\n",
       "1                              2.Iruvar          1997    8.4\n",
       "2                     3.Kaagaz Ke Phool          1959    7.8\n",
       "3   4.Lagaan: Once Upon a Time in India          2001    8.1\n",
       "4                     5.Pather Panchali          1955    8.2\n",
       "..                                  ...           ...    ...\n",
       "95                       96.Apur Sansar          1959    8.5\n",
       "96                       97.Kanchivaram          2008    8.2\n",
       "97                   98.Monsoon Wedding          2001    7.3\n",
       "98                             99.Black          2005    8.1\n",
       "99                          100.Deewaar          1975      8\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "MovieData=pd.DataFrame({'MovieNames': MovieNames,'YearOfRelease':Year,'Rating':Rating})\n",
    "MovieData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27b604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c8fba5",
   "metadata": {},
   "source": [
    "4)Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706e7454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed HTML content from https://www.patreon.com/coreyms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        pateron = BeautifulSoup(response.content, 'html.parser')\n",
    "        return pateron\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "pateron = parse_html(url)\n",
    "if pateron:\n",
    "    print(f\"Successfully parsed HTML content from {url}\")\n",
    "else:\n",
    "    print(f\"Failed to parse HTML content from {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b250523",
   "metadata": {},
   "source": [
    "Scraping heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9c14ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading=[]\n",
    "\n",
    "for i in pateron.find_all('div',class_='sc-bBHxTw iloeMK'):\n",
    "    heading.append(i.text)\n",
    "    \n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ca73d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date=[]\n",
    "\n",
    "for i in pateron.find_all('div',class_=\"sc-lgu5zg-0 dXpjXs\"):\n",
    "       date.append(i.text)\n",
    "date\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490e02c",
   "metadata": {},
   "source": [
    "likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587b742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes=[]\n",
    "for i in pateron.find_all('div',class_='sc-jrQzAO cENdHk'):\n",
    "     likes.append(i.text)\n",
    "likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26124370",
   "metadata": {},
   "source": [
    "5)Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e659f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed HTML content from https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciJ9LHsibGF0IjoxMi45MzA3NzM1LCJsb24iOjc3LjU4MzgzMDIsInBsYWNlSWQiOiJDaElKMmRkbFo1Z1ZyanNSaDFCT0FhZi1vcnMiLCJwbGFjZU5hbWUiOiJKYXlhbmFnYXIifSx7ImxhdCI6MTIuOTk4MTczMiwibG9uIjo3Ny41NTMwNDQ1OTk5OTk5OSwicGxhY2VJZCI6IkNoSUp4Zlc0RFBNOXJqc1JLc05URy01cF9RUSIsInBsYWNlTmFtZSI6IlJhamFqaW5hZ2FyIn1d&radius=2.0&city=bangalore&locality=Indiranagar,Jayanagar,Rajajinagar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        nobroker = BeautifulSoup(response.content, 'html.parser')\n",
    "        return nobroker\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciJ9LHsibGF0IjoxMi45MzA3NzM1LCJsb24iOjc3LjU4MzgzMDIsInBsYWNlSWQiOiJDaElKMmRkbFo1Z1ZyanNSaDFCT0FhZi1vcnMiLCJwbGFjZU5hbWUiOiJKYXlhbmFnYXIifSx7ImxhdCI6MTIuOTk4MTczMiwibG9uIjo3Ny41NTMwNDQ1OTk5OTk5OSwicGxhY2VJZCI6IkNoSUp4Zlc0RFBNOXJqc1JLc05URy01cF9RUSIsInBsYWNlTmFtZSI6IlJhamFqaW5hZ2FyIn1d&radius=2.0&city=bangalore&locality=Indiranagar,Jayanagar,Rajajinagar\"\n",
    "nobroker = parse_html(url)\n",
    "if nobroker:\n",
    "    print(f\"Successfully parsed HTML content from {url}\")\n",
    "else:\n",
    "    print(f\"Failed to parse HTML content from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127aa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraping House title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84fc2d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4 BHK House For Sale  In Indiranagar',\n",
       " '4 BHK House For Sale  In Rajajinagar',\n",
       " '3 BHK Flat In Gokula Krishna Apartment For Sale  In Basavanagudi',\n",
       " '3 BHK Apartment In Gokul Lake View Apartments For Sale  In Jayanagar',\n",
       " '3 BHK Flat In Santa Clara Apartment For Sale  In Marenahalli, Jayanagar',\n",
       " '3 BHK Flat In Ltg Sr Grand For Sale  In Rajaji Nagar',\n",
       " '3 BHK Apartment In Amarjyothi Apartments For Sale  In Jayanagar 6th Block, Jayanagar',\n",
       " '4+ BHK House For Sale  In  Rajajinagar',\n",
       " '3 BHK Flat In Suraj Lakshmi Apartments For Sale  In Basavanagudi',\n",
       " '3 BHK Flat For Sale  In Rajajinagar',\n",
       " '3 BHK House For Sale  In Jayanagar',\n",
       " '4+ BHK House For Sale  In Rajajinagar',\n",
       " '4 BHK House For Sale  In Nandini Layout',\n",
       " '3 BHK Flat In Sri Maruthi For Sale  In Jayanagar',\n",
       " '2 BHK Flat In Pallava Terrace Apartments For Sale  In Jayanagar',\n",
       " '3 BHK Flat In Kashi Kiran Regency, Rajajinagar For Sale  In Rajajinagar',\n",
       " '3 BHK Apartment In Jayanagar Residency For Sale  In Jayanagar',\n",
       " '2 BHK Apartment In Alpine Regency For Sale  In Jayanagar',\n",
       " '4 BHK Flat In Apartment For Sale  In Rajajinagar',\n",
       " '3 BHK Flat In Mayitri Enclave For Sale  In Jayanagar',\n",
       " '2 BHK Apartment In Gopalan Admirality Court, Indiranagar For Sale  In Indiranagar',\n",
       " '3 BHK Flat In Hemadri Residency For Sale  In Rajajinagar',\n",
       " '4+ BHK House For Sale  In 183, 7th Cross Rd, 2nd Stage, Rajajinagar, Bengaluru, Karnataka 560010, India',\n",
       " '3 BHK Apartment In Codename Indiranagar For Sale  In Indiranagar',\n",
       " '4 BHK Apartment In Codename Indiranagar For Sale  In Indiranagar',\n",
       " '4 BHK House For Sale  In Indiranagar']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HouseTitle=[]\n",
    "for i in nobroker.find_all('a',{'class':'overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full'}):\n",
    "      HouseTitle.append(i.text)\n",
    "HouseTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56e98e",
   "metadata": {},
   "source": [
    "Scraping Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dd90cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Independent House,  100 Feet Rd Near New Horizon Public School',\n",
       " \"Independent House, Mahakavi Kuvempu Rd Maruthi Extension,Near Malgudi's Donne Biriyani\",\n",
       " 'South End Rd near Reliance Fresh',\n",
       " 'Gokul Lake View Apartments\\xa0 Gokul Lake View Apartments 110/18, 19th Cross Rd 2nd A Main Rd, Jayanagar Bengaluru, Karnataka 560070 India',\n",
       " '3Rd Cross Rd18th main, 9th Block, Marenahalli',\n",
       " 'west of card road near modi hospital ',\n",
       " 'Amarjyothi Apartments\\xa0 Jayanagar 6th block, Bangalore',\n",
       " \"Independent House, 80 Feet Rd 6th Block, near St. Ann's High School\",\n",
       " '34/1, 1st Main Rd, Tata Silk Farm, Basavanagudi, Bengaluru, Karnataka 560004, India',\n",
       " \"Standalone Building, 6th Block, near St. Ann's High School\",\n",
       " 'Independent House, Pattabhirama Nagar Near  Sanjay Gandhi Hospital',\n",
       " 'Independent House, SPECTRUM DIAGNOSTICS AND HEALTHCARE',\n",
       " 'Independent House\\xa0 Independent House, Independent House, 1st Main, Dollar Scheme',\n",
       " 'Indira Gandhi Institute Of Child Health 1st Block, Jayanagar, Bengaluru, Karnataka 560029, India',\n",
       " 'Pallava Terrace Tata Silk Farm, Jayanagar, Bengaluru, Karnataka 560070',\n",
       " 'Mariappanapalya, Rajajinagar, Bengaluru, Karnataka 560021, India',\n",
       " 'Jayanagar Residency\\xa0 Jayanagar  Jayanagar Residency',\n",
       " 'Alpine Regency\\xa0  Alpine Regency, 10th D Main Rd, 1st Block, Jaya Nagar East, Jayanagar, Bengaluru, Karnataka 560011, India',\n",
       " 'mahakavi kuvempu road, Near Mahakavi Kuvempu metro station',\n",
       " 'Mayitri Enclave,5T Block, 4th T Block East, Jayanagar, Bengaluru, Karnataka 5600041, India',\n",
       " 'Gopalan Admirality Court, Indiranagar\\xa0  6th Main Rd, Eshwara Layout, Indiranagar, Bengaluru, Karnataka 560038, India',\n",
       " '80 Feet Rd, 4th Block, Rajajinagar, Bengaluru, Karnataka 560010, India',\n",
       " 'Independent House, 7th Cross First N Block',\n",
       " 'Codename Indiranagar\\xa0 Indiranagar, Bangalore.',\n",
       " 'Codename Indiranagar\\xa0 Indiranagar, Bangalore.',\n",
       " 'Independent House,  100 Feet Rd Near New Horizon Public School']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area=[]\n",
    "for i in nobroker.find_all(\"div\",class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0.1p po:max-w-95\"):\n",
    "          area.append(i.text)                 \n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Price column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd2dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹3.5 Crores',\n",
       " '₹2.01 Lacs/Month',\n",
       " '4,800 sqft',\n",
       " '₹1.1 Crores',\n",
       " '₹63,045/Month',\n",
       " '2,000 sqft',\n",
       " '₹2.5 Crores',\n",
       " '₹1.43 Lacs/Month',\n",
       " '2,100 sqft',\n",
       " '₹1.62 Crores',\n",
       " '₹92,849/Month',\n",
       " '1,852 sqft',\n",
       " '₹1.15 Crores',\n",
       " '₹65,911/Month',\n",
       " '1,395 sqft',\n",
       " '₹2.3 Crores',\n",
       " '₹1.32 Lacs/Month',\n",
       " '1,865 sqft',\n",
       " '₹1.69 Crores',\n",
       " '₹96,861/Month',\n",
       " '1,552 sqft',\n",
       " '₹1.25 Crores',\n",
       " '₹71,643/Month',\n",
       " '2,400 sqft',\n",
       " '₹1.2 Crores',\n",
       " '₹68,777/Month',\n",
       " '1,226 sqft',\n",
       " '₹1.26 Crores',\n",
       " '₹72,216/Month',\n",
       " '1,100 sqft',\n",
       " '₹6.9 Crores',\n",
       " '₹3.95 Lacs/Month',\n",
       " '4,000 sqft',\n",
       " '₹4.17 Crores',\n",
       " '₹2.39 Lacs/Month',\n",
       " '2,700 sqft',\n",
       " '₹12 Crores',\n",
       " '₹6.88 Lacs/Month',\n",
       " '2,400 sqft',\n",
       " '₹1.2 Crores',\n",
       " '₹68,777/Month',\n",
       " '1,400 sqft',\n",
       " '₹1.4 Crores',\n",
       " '₹80,240/Month',\n",
       " '1,050 sqft',\n",
       " '₹1 Crore',\n",
       " '₹57,314/Month',\n",
       " '1,200 sqft',\n",
       " '₹2.25 Crores',\n",
       " '₹1.29 Lacs/Month',\n",
       " '2,000 sqft',\n",
       " '₹1.65 Crores',\n",
       " '₹94,568/Month',\n",
       " '1,320 sqft',\n",
       " '₹1.8 Crores',\n",
       " '₹1.03 Lacs/Month',\n",
       " '2,055 sqft',\n",
       " '₹85 Lacs',\n",
       " '₹48,717/Month',\n",
       " '1,350 sqft',\n",
       " '₹1.82 Crores',\n",
       " '₹1.04 Lacs/Month',\n",
       " '1,360 sqft',\n",
       " '₹1.8 Crores',\n",
       " '₹1.03 Lacs/Month',\n",
       " '2,090 sqft',\n",
       " '₹5.2 Crores',\n",
       " '₹2.98 Lacs/Month',\n",
       " '2,200 sqft',\n",
       " '₹5.6 Crores',\n",
       " '₹3.21 Lacs/Month',\n",
       " '2,000 sqft',\n",
       " '₹6 Crores',\n",
       " '₹3.44 Lacs/Month',\n",
       " '2,600 sqft',\n",
       " '₹3.5 Crores',\n",
       " '₹2.01 Lacs/Month',\n",
       " '4,800 sqft']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "Price=[]\n",
    "\n",
    "for i in nobroker.find_all('div',class_='font-semi-bold heading-6'):\n",
    "       Price.append(i.text)\n",
    "                           \n",
    "Price\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70efeb8",
   "metadata": {},
   "source": [
    "Extracting Price from Scraped Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45699ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.5 Crores']\n",
      "['1.1 Crores']\n",
      "['2.5 Crores']\n",
      "['1.62 Crores']\n",
      "['1.15 Crores']\n",
      "['2.3 Crores']\n",
      "['1.69 Crores']\n",
      "['1.25 Crores']\n",
      "['1.2 Crores']\n",
      "['1.26 Crores']\n",
      "['6.9 Crores']\n",
      "['4.17 Crores']\n",
      "['1.2 Crores']\n",
      "['1.4 Crores']\n",
      "['2.25 Crores']\n",
      "['1.65 Crores']\n",
      "['1.8 Crores']\n",
      "['1.82 Crores']\n",
      "['1.8 Crores']\n",
      "['5.2 Crores']\n",
      "['5.6 Crores']\n",
      "['3.5 Crores']\n"
     ]
    }
   ],
   "source": [
    "HousePrice=[]\n",
    "\n",
    "for n in Price:\n",
    "    HousePrice=re.findall(\"\\d.\\d+ Crores\",n)\n",
    "    for m in HousePrice:\n",
    "        print(HousePrice)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "523cd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extracting  EMI from Scraped Price\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "170e1905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2.01 Lacs/Month',)]\n",
      "[('63,045/Month',)]\n",
      "[('1.43 Lacs/Month',)]\n",
      "[('92,849/Month',)]\n",
      "[('65,911/Month',)]\n",
      "[('1.32 Lacs/Month',)]\n",
      "[('96,861/Month',)]\n",
      "[('71,643/Month',)]\n",
      "[('68,777/Month',)]\n",
      "[('72,216/Month',)]\n",
      "[('3.95 Lacs/Month',)]\n",
      "[('2.39 Lacs/Month',)]\n",
      "[('6.88 Lacs/Month',)]\n",
      "[('68,777/Month',)]\n",
      "[('80,240/Month',)]\n",
      "[('57,314/Month',)]\n",
      "[('1.29 Lacs/Month',)]\n",
      "[('94,568/Month',)]\n",
      "[('1.03 Lacs/Month',)]\n",
      "[('48,717/Month',)]\n",
      "[('1.04 Lacs/Month',)]\n",
      "[('1.03 Lacs/Month',)]\n",
      "[('2.98 Lacs/Month',)]\n",
      "[('3.21 Lacs/Month',)]\n",
      "[('3.44 Lacs/Month',)]\n",
      "[('2.01 Lacs/Month',)]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "for m in Price:\n",
    "    EMI_list=re.findall(\"(?=(\\d.\\d+ Lacs/Month))|(?=(\\d{2},\\d+/Month))\",m)\n",
    "    EMI = [tuple(filter(lambda x: x != \"\", tpl)) for tpl in EMI_list]\n",
    "    for s in EMI:\n",
    "        print(EMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb0c3b",
   "metadata": {},
   "source": [
    "Scraping Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edf8fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IndiranagarJayanagarRajajinagar']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc=[]\n",
    "for i in nobroker.find_all('span',class_='listpage-serach-selected-localities'):\n",
    "       loc.append(i.text)\n",
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588422e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12261703",
   "metadata": {},
   "source": [
    "6)Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "https://www.bewakoof.com/bestseller?sort=popular ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "49ad836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed HTML content from https://www.bewakoof.com/bestseller?sort=popular/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        bewakoof = BeautifulSoup(response.content, 'html.parser')\n",
    "        return bewakoof\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular/\"\n",
    "bewakoof = parse_html(url)\n",
    "if bewakoof:\n",
    "    print(f\"Successfully parsed HTML content from {url}\")\n",
    "else:\n",
    "    print(f\"Failed to parse HTML content from {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b72f3",
   "metadata": {},
   "source": [
    "Scraping ProductName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87d3b94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Women's Brown Cargo Pants\",\n",
       " \"Women's Blue Straight Cargo Pants\",\n",
       " \"Women's Black Straight Cargo Pants\",\n",
       " \"Women's Brown Cargo Pants\",\n",
       " \"Women's Brown Straight Cargo Pants\",\n",
       " \"Women's White Camo Printed Oversized Short Top\",\n",
       " \"bewakoof x garfieldWomen's White All Over I Hate Mondays Printed Oversized Short Top\",\n",
       " \"Women's Black All Over Printed Oversized Short Top\",\n",
       " \"Women's Pink & White Camo Printed Oversized Short Top\",\n",
       " \"Women's White Bored Typography Oversized Short Top\"]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProductName=[]\n",
    "for i in bewakoof.find_all('div',class_='productNaming bkf-ellipsis')[:10]:\n",
    "     ProductName.append(i.text.replace(\"Bewakoof®\",\"\"))\n",
    "ProductName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0161715",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraping Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ddd5d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1199', '1199', '1299', '999', '1399', '519', '399', '499', '599', '499']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Price=[]\n",
    "for i in bewakoof.find_all('div',class_='discountedPriceText')[:10]:\n",
    "    Price.append(i.text.replace('₹',''))\n",
    "Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraping ImageURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b0e81aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://images.bewakoof.com/t640/women-s-straight-cargo-pants-13-620015-1702644202-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-s-blue-straight-cargo-pants-620014-1708329141-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-s-black-straight-cargo-pants-620005-1708328865-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-s-brown-cargo-pants-585718-1708328826-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-s-brown-straight-cargo-pants-620020-1708328904-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-aop-oversize-t-shirt-4-580364-1683887449-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-aop-oversized-t-shirt-11-582004-1685446587-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-aop-oversize-t-shirt-3-580366-1682421809-1.JPG',\n",
       " 'https://images.bewakoof.com/t640/women-s-pink-white-camo-printed-oversized-t-shirt-580369-1686301190-1.jpg',\n",
       " 'https://images.bewakoof.com/t640/women-white-printed-top-26-582038-1689082446-1.jpg']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Images=[]\n",
    "for i in bewakoof.find_all('img',class_='productImgTag')[:10]:\n",
    "     Images.append(i['src'])\n",
    "Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6fc2f942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>ImageURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women's Brown Cargo Pants</td>\n",
       "      <td>1199</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-s-strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women's Blue Straight Cargo Pants</td>\n",
       "      <td>1199</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-s-blue-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women's Black Straight Cargo Pants</td>\n",
       "      <td>1299</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-s-black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women's Brown Cargo Pants</td>\n",
       "      <td>999</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-s-brown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women's Brown Straight Cargo Pants</td>\n",
       "      <td>1399</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-s-brown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Women's White Camo Printed Oversized Short Top</td>\n",
       "      <td>519</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-aop-ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bewakoof x garfieldWomen's White All Over I Ha...</td>\n",
       "      <td>399</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-aop-ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Women's Black All Over Printed Oversized Short...</td>\n",
       "      <td>499</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-aop-ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Women's Pink &amp; White Camo Printed Oversized Sh...</td>\n",
       "      <td>599</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-s-pink-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Women's White Bored Typography Oversized Short...</td>\n",
       "      <td>499</td>\n",
       "      <td>https://images.bewakoof.com/t640/women-white-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ProductName Price  \\\n",
       "0                          Women's Brown Cargo Pants  1199   \n",
       "1                  Women's Blue Straight Cargo Pants  1199   \n",
       "2                 Women's Black Straight Cargo Pants  1299   \n",
       "3                          Women's Brown Cargo Pants   999   \n",
       "4                 Women's Brown Straight Cargo Pants  1399   \n",
       "5     Women's White Camo Printed Oversized Short Top   519   \n",
       "6  bewakoof x garfieldWomen's White All Over I Ha...   399   \n",
       "7  Women's Black All Over Printed Oversized Short...   499   \n",
       "8  Women's Pink & White Camo Printed Oversized Sh...   599   \n",
       "9  Women's White Bored Typography Oversized Short...   499   \n",
       "\n",
       "                                            ImageURL  \n",
       "0  https://images.bewakoof.com/t640/women-s-strai...  \n",
       "1  https://images.bewakoof.com/t640/women-s-blue-...  \n",
       "2  https://images.bewakoof.com/t640/women-s-black...  \n",
       "3  https://images.bewakoof.com/t640/women-s-brown...  \n",
       "4  https://images.bewakoof.com/t640/women-s-brown...  \n",
       "5  https://images.bewakoof.com/t640/women-aop-ove...  \n",
       "6  https://images.bewakoof.com/t640/women-aop-ove...  \n",
       "7  https://images.bewakoof.com/t640/women-aop-ove...  \n",
       "8  https://images.bewakoof.com/t640/women-s-pink-...  \n",
       "9  https://images.bewakoof.com/t640/women-white-p...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bewakoofData=pd.DataFrame({\"ProductName\":ProductName,\"Price\":Price,\"ImageURL\":Images})\n",
    "bewakoofData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c00fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c765bab",
   "metadata": {},
   "source": [
    "7)Please visit https://www.cnbc.com/world/?region=world and scrap\u0002a) headings\n",
    "b) date\n",
    "c) News lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "46e18a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed HTML content from https://www.cnbc.com/world/?region=world/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parse_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        cnbc = BeautifulSoup(response.content, 'html.parser')\n",
    "        return cnbc\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world/\"\n",
    "cnbc = parse_html(url)\n",
    "if cnbc:\n",
    "    print(f\"Successfully parsed HTML content from {url}\")\n",
    "else:\n",
    "    print(f\"Failed to parse HTML content from {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6a6f6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a8b3811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Markets',\n",
       " 'Business',\n",
       " 'Investing',\n",
       " 'Tech',\n",
       " 'Politics',\n",
       " 'CNBC TV',\n",
       " 'Watchlist',\n",
       " 'Investing Club',\n",
       " 'PRO',\n",
       " 'Menu']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings=[]\n",
    "for i in cnbc.find_all('span',class_='nav-menu-buttonText'):\n",
    "    headings.append(i.text)\n",
    "headings\n",
    "\n",
    "res=set()\n",
    "title=[]\n",
    "for s in headings:\n",
    "      if s not in res:\n",
    "        res.add(s)\n",
    "        title.append(s)\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab069c1",
   "metadata": {},
   "source": [
    "Scraping newslin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d66d47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Zelenskyy says U.S. aid delay let Russia grab initiative'; Blinken warns of sanctions on China firms\",\n",
       " \"Goldman likes these 2 mobile gaming stocks set for 'blockbuster' launches, gives one 40% upside\",\n",
       " 'Airbus CFO says A350 plane production increase not tied to Boeing troubles',\n",
       " 'Elliott takes $1 billion stake in Anglo American as miner faces takeover interest',\n",
       " \"Anglo American rejects BHP's $39 billion takeover bid to form mining juggernaut\",\n",
       " 'Long-awaited $2 billion CVC debut shows the IPO market is back on track, Euronext boss says',\n",
       " \"Should investors buy the dip in Lululemon? Here's what this fund manager says\",\n",
       " 'Meet the Dubai artist whose work has sold for millions — and turns down 99% of prospective buyers',\n",
       " 'Venice residents clash with riot police as city launches world’s first tourist entry fee',\n",
       " \"China's Xi says the U.S. needs to accept Beijing's rise for bilateral relations to improve\"]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newslin=[]\n",
    "for i in cnbc.find_all('div',class_='RiverHeadline-headline RiverHeadline-hasThumbnail')[:10]:\n",
    "      newslin.append(i.text)\n",
    "newslin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a66b7",
   "metadata": {},
   "source": [
    "Scraping Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1f957960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['41 min ago',\n",
       " '41 min ago',\n",
       " '3 hours ago',\n",
       " '3 hours ago',\n",
       " 'an hour ago',\n",
       " 'an hour ago',\n",
       " '5 hours ago',\n",
       " '5 hours ago',\n",
       " '5 hours ago',\n",
       " '5 hours ago']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date=[]\n",
    "for i in cnbc.find_all('span',class_='RiverByline-datePublished')[:10]:\n",
    "     date.append(i.text)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d9ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4fa726a",
   "metadata": {},
   "source": [
    "8)Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded\u0002articles/ and scrap-\n",
    " a) Paper title\n",
    " b) date\n",
    " c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e22ba9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed HTML content from https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        publishing = BeautifulSoup(response.content, 'html.parser')\n",
    "        return publishing\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "publishing = parse_html(url)\n",
    "if publishing:\n",
    "    print(f\"Successfully parsed HTML content from {url}\")\n",
    "else:\n",
    "    print(f\"Failed to parse HTML content from {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfc1d6",
   "metadata": {},
   "source": [
    "Scraping Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3d87f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Implementation of artificial intelligence in agriculture for optimisation of irrigation and application of pesticides and herbicides ',\n",
       " 'A comprehensive review on automation in agriculture using artificial intelligence ',\n",
       " 'Review of agricultural IoT technology ',\n",
       " 'Automation and digitization of agriculture using artificial intelligence and internet of things ',\n",
       " 'Real-time hyperspectral imaging for the in-field estimation of strawberry ripeness with deep learning ',\n",
       " 'A review of imaging techniques for plant disease detection ',\n",
       " 'Applications of electronic nose (e-nose) and electronic tongue (e-tongue) in food quality-related properties determination: A review ',\n",
       " 'Fruit ripeness classification: A survey ',\n",
       " 'Deep learning based computer vision approaches for smart agricultural applications ',\n",
       " 'DeepRice: A deep learning and deep feature based classification of Rice leaf disease subtypes ',\n",
       " 'Transfer Learning for Multi-Crop Leaf Disease Image Classification using Convolutional Neural Network VGG ',\n",
       " 'How artificial intelligence uses to achieve the agriculture sustainability: Systematic review ',\n",
       " 'Comparison of CNN-based deep learning architectures for rice diseases classification ',\n",
       " 'Plant disease detection using hybrid model based on convolutional autoencoder and convolutional neural network ',\n",
       " 'Deep convolutional neural network models for weed detection in polyhouse grown bell peppers ',\n",
       " 'Examining the interplay between artificial intelligence and the agri-food industry ',\n",
       " 'Machine learning in nutrient management: A review ',\n",
       " 'A systematic review of machine learning techniques for cattle identification: Datasets, methods and future directions ',\n",
       " 'A review on computer vision systems in monitoring of poultry: A welfare perspective ',\n",
       " 'Machine learning for weed–plant discrimination in agriculture 5.0: An in-depth review ',\n",
       " 'Artificial cognition for applications in smart agriculture: A comprehensive review ',\n",
       " 'Vision Intelligence for Smart Sheep Farming: Applying Ensemble Learning to Detect Sheep Breeds ',\n",
       " 'Explainable artificial intelligence and interpretable machine learning for agricultural data analysis ',\n",
       " 'Crop diagnostic system: A robust disease detection and management system for leafy green crops grown in an aquaponics facility ',\n",
       " 'Automated quality inspection of baby corn using image processing and deep learning ']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PaperTitle = []\n",
    "for i in publishing.find_all('h2',class_='h5 article-title'):\n",
    "      PaperTitle.append(i.text.replace('\\r','').replace('\\n','').replace('     ',''))\n",
    "        \n",
    "PaperTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraping Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c5ee7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020',\n",
       " 'June 2019',\n",
       " '2022',\n",
       " '2021',\n",
       " '2020',\n",
       " '2020',\n",
       " '2020',\n",
       " 'March 2023',\n",
       " '2022',\n",
       " 'March 2024',\n",
       " '2022',\n",
       " 'June 2023',\n",
       " 'September 2023',\n",
       " '2021',\n",
       " '2022',\n",
       " '2022',\n",
       " 'September 2023',\n",
       " '2022',\n",
       " '2020',\n",
       " 'December 2023',\n",
       " '2020',\n",
       " 'March 2024',\n",
       " '2022',\n",
       " 'December 2023',\n",
       " 'March 2024']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Date=[]\n",
    "for i in publishing.find_all('p',class_='article-date'):\n",
    "      Date.append(i.text)\n",
    "\n",
    "Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d0069",
   "metadata": {},
   "source": [
    "Scraping Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8f1ea846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Tanha Talaviya |  Dhara Shah |  Nivedita Patel |  Hiteshri Yagnik |  Manan Shah',\n",
       " ' Kirtan Jha |  Aalap Doshi |  Poojan Patel |  Manan Shah',\n",
       " ' Jinyuan Xu |  Baoxing Gu |  Guangzhao Tian',\n",
       " ' A. Subeesh |  C.R. Mehta',\n",
       " ' Zongmei Gao |  Yuanyuan Shao |  Guantao Xuan |  Yongxian Wang |  Yi Liu |  Xiang Han',\n",
       " ' Vijai Singh |  Namita Sharma |  Shikha Singh',\n",
       " ' Juzhong Tan |  Jie Xu',\n",
       " ' Matteo Rizzo |  Matteo Marcuzzo |  Alessandro Zangari |  Andrea Gasparetto |  Andrea Albarelli',\n",
       " ' V.G. Dhanya |  A. Subeesh |  N.L. Kushwaha |  Dinesh Kumar Vishwakarma |  T. Nagesh Kumar |  G. Ritika |  A.N. Singh',\n",
       " ' P. Isaac Ritharson |  Kumudha Raimond |  X. Anitha Mary |  Jennifer Eunice Robert |  Andrew J',\n",
       " ' Ananda S. Paymode |  Vandana B. Malode',\n",
       " ' Vilani Sachithra |  L.D.C.S. Subhashini',\n",
       " ' Md Taimur Ahad |  Yan Li |  Bo Song |  Touhid Bhuiyan',\n",
       " ' Punam Bedi |  Pushkar Gole',\n",
       " ' A. Subeesh |  S. Bhole |  K. Singh |  N.S. Chandel |  Y.A. Rajwade |  K.V.R. Rao |  S.P. Kumar |  D. Jat',\n",
       " ' Abderahman Rejeb |  Karim Rejeb |  Suhaiza Zailani |  John G. Keogh |  Andrea Appolloni',\n",
       " ' Oumnia Ennaji |  Leonardus Vergütz |  Achraf El Allali',\n",
       " ' Md Ekramul Hossain |  Muhammad Ashad Kabir |  Lihong Zheng |  Dave L. Swain |  Shawn McGrath |  Jonathan Medway',\n",
       " ' Cedric Okinda |  Innocent Nyalala |  Tchalla Korohou |  Celestine Okinda |  Jintao Wang |  Tracy Achieng |  Patrick Wamalwa |  Tai Mang |  Mingxia Shen',\n",
       " ' Filbert H. Juwono |  W.K. Wong |  Seema Verma |  Neha Shekhawat |  Basil Andy Lease |  Catur Apriono',\n",
       " ' Misbah Pathan |  Nivedita Patel |  Hiteshri Yagnik |  Manan Shah',\n",
       " ' Galib Muhammad Shahriar Himel |  Md. Masudul Islam |  Mijanur Rahaman',\n",
       " ' Masahiro Ryo',\n",
       " ' R. Abbasi |  P. Martinez |  R. Ahmad',\n",
       " ' Kris Wonggasem |  Pongsan Chakranon |  Papis Wongchaisuwat']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Author=[]\n",
    "for i in publishing.find_all('p','article-authors'):\n",
    "     Author.append(i.text)\n",
    "Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dbc5eb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PaperTitle</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Implementation of artificial intelligence in a...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Tanha Talaviya |  Dhara Shah |  Nivedita Pate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A comprehensive review on automation in agricu...</td>\n",
       "      <td>June 2019</td>\n",
       "      <td>Kirtan Jha |  Aalap Doshi |  Poojan Patel |  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review of agricultural IoT technology</td>\n",
       "      <td>2022</td>\n",
       "      <td>Jinyuan Xu |  Baoxing Gu |  Guangzhao Tian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automation and digitization of agriculture usi...</td>\n",
       "      <td>2021</td>\n",
       "      <td>A. Subeesh |  C.R. Mehta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Real-time hyperspectral imaging for the in-fie...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Zongmei Gao |  Yuanyuan Shao |  Guantao Xuan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A review of imaging techniques for plant disea...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Vijai Singh |  Namita Sharma |  Shikha Singh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Applications of electronic nose (e-nose) and e...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Juzhong Tan |  Jie Xu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fruit ripeness classification: A survey</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>Matteo Rizzo |  Matteo Marcuzzo |  Alessandro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deep learning based computer vision approaches...</td>\n",
       "      <td>2022</td>\n",
       "      <td>V.G. Dhanya |  A. Subeesh |  N.L. Kushwaha | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DeepRice: A deep learning and deep feature bas...</td>\n",
       "      <td>March 2024</td>\n",
       "      <td>P. Isaac Ritharson |  Kumudha Raimond |  X. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Transfer Learning for Multi-Crop Leaf Disease ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Ananda S. Paymode |  Vandana B. Malode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How artificial intelligence uses to achieve th...</td>\n",
       "      <td>June 2023</td>\n",
       "      <td>Vilani Sachithra |  L.D.C.S. Subhashini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Comparison of CNN-based deep learning architec...</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>Md Taimur Ahad |  Yan Li |  Bo Song |  Touhid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Plant disease detection using hybrid model bas...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Punam Bedi |  Pushkar Gole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Deep convolutional neural network models for w...</td>\n",
       "      <td>2022</td>\n",
       "      <td>A. Subeesh |  S. Bhole |  K. Singh |  N.S. Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Examining the interplay between artificial int...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Abderahman Rejeb |  Karim Rejeb |  Suhaiza Za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Machine learning in nutrient management: A rev...</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>Oumnia Ennaji |  Leonardus Vergütz |  Achraf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A systematic review of machine learning techni...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Md Ekramul Hossain |  Muhammad Ashad Kabir | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review on computer vision systems in monitor...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Cedric Okinda |  Innocent Nyalala |  Tchalla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Machine learning for weed–plant discrimination...</td>\n",
       "      <td>December 2023</td>\n",
       "      <td>Filbert H. Juwono |  W.K. Wong |  Seema Verma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Artificial cognition for applications in smart...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Misbah Pathan |  Nivedita Patel |  Hiteshri Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Vision Intelligence for Smart Sheep Farming: A...</td>\n",
       "      <td>March 2024</td>\n",
       "      <td>Galib Muhammad Shahriar Himel |  Md. Masudul ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Explainable artificial intelligence and interp...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Masahiro Ryo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Crop diagnostic system: A robust disease detec...</td>\n",
       "      <td>December 2023</td>\n",
       "      <td>R. Abbasi |  P. Martinez |  R. Ahmad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Automated quality inspection of baby corn usin...</td>\n",
       "      <td>March 2024</td>\n",
       "      <td>Kris Wonggasem |  Pongsan Chakranon |  Papis ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           PaperTitle            Date  \\\n",
       "0   Implementation of artificial intelligence in a...            2020   \n",
       "1   A comprehensive review on automation in agricu...       June 2019   \n",
       "2              Review of agricultural IoT technology             2022   \n",
       "3   Automation and digitization of agriculture usi...            2021   \n",
       "4   Real-time hyperspectral imaging for the in-fie...            2020   \n",
       "5   A review of imaging techniques for plant disea...            2020   \n",
       "6   Applications of electronic nose (e-nose) and e...            2020   \n",
       "7            Fruit ripeness classification: A survey       March 2023   \n",
       "8   Deep learning based computer vision approaches...            2022   \n",
       "9   DeepRice: A deep learning and deep feature bas...      March 2024   \n",
       "10  Transfer Learning for Multi-Crop Leaf Disease ...            2022   \n",
       "11  How artificial intelligence uses to achieve th...       June 2023   \n",
       "12  Comparison of CNN-based deep learning architec...  September 2023   \n",
       "13  Plant disease detection using hybrid model bas...            2021   \n",
       "14  Deep convolutional neural network models for w...            2022   \n",
       "15  Examining the interplay between artificial int...            2022   \n",
       "16  Machine learning in nutrient management: A rev...  September 2023   \n",
       "17  A systematic review of machine learning techni...            2022   \n",
       "18  A review on computer vision systems in monitor...            2020   \n",
       "19  Machine learning for weed–plant discrimination...   December 2023   \n",
       "20  Artificial cognition for applications in smart...            2020   \n",
       "21  Vision Intelligence for Smart Sheep Farming: A...      March 2024   \n",
       "22  Explainable artificial intelligence and interp...            2022   \n",
       "23  Crop diagnostic system: A robust disease detec...   December 2023   \n",
       "24  Automated quality inspection of baby corn usin...      March 2024   \n",
       "\n",
       "                                               Author  \n",
       "0    Tanha Talaviya |  Dhara Shah |  Nivedita Pate...  \n",
       "1    Kirtan Jha |  Aalap Doshi |  Poojan Patel |  ...  \n",
       "2          Jinyuan Xu |  Baoxing Gu |  Guangzhao Tian  \n",
       "3                            A. Subeesh |  C.R. Mehta  \n",
       "4    Zongmei Gao |  Yuanyuan Shao |  Guantao Xuan ...  \n",
       "5        Vijai Singh |  Namita Sharma |  Shikha Singh  \n",
       "6                               Juzhong Tan |  Jie Xu  \n",
       "7    Matteo Rizzo |  Matteo Marcuzzo |  Alessandro...  \n",
       "8    V.G. Dhanya |  A. Subeesh |  N.L. Kushwaha | ...  \n",
       "9    P. Isaac Ritharson |  Kumudha Raimond |  X. A...  \n",
       "10             Ananda S. Paymode |  Vandana B. Malode  \n",
       "11            Vilani Sachithra |  L.D.C.S. Subhashini  \n",
       "12   Md Taimur Ahad |  Yan Li |  Bo Song |  Touhid...  \n",
       "13                         Punam Bedi |  Pushkar Gole  \n",
       "14   A. Subeesh |  S. Bhole |  K. Singh |  N.S. Ch...  \n",
       "15   Abderahman Rejeb |  Karim Rejeb |  Suhaiza Za...  \n",
       "16   Oumnia Ennaji |  Leonardus Vergütz |  Achraf ...  \n",
       "17   Md Ekramul Hossain |  Muhammad Ashad Kabir | ...  \n",
       "18   Cedric Okinda |  Innocent Nyalala |  Tchalla ...  \n",
       "19   Filbert H. Juwono |  W.K. Wong |  Seema Verma...  \n",
       "20   Misbah Pathan |  Nivedita Patel |  Hiteshri Y...  \n",
       "21   Galib Muhammad Shahriar Himel |  Md. Masudul ...  \n",
       "22                                       Masahiro Ryo  \n",
       "23               R. Abbasi |  P. Martinez |  R. Ahmad  \n",
       "24   Kris Wonggasem |  Pongsan Chakranon |  Papis ...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "KeaiPub=pd.DataFrame({\"PaperTitle\":PaperTitle,\"Date\":Date,\"Author\":Author})\n",
    "KeaiPub                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e1b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
